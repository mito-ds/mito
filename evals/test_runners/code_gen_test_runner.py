import pprint
from typing import Dict, List, Literal, Optional, Union
from evals.ai_api_calls.get_open_ai_completion import get_open_ai_completion
from evals.asserts.equal_outputs import assert_equal_outputs
from evals.eval_types import ChatPromptGenerator, ChatTestCase, InlineCodeCompletionPromptGenerator, InlineCodeCompletionTestCase, TestCaseResult
from evals.prompts.chat_prompts import CHAT_PROMPT_GENERATORS
from evals.prompts.inline_code_completion_prompts import INLINE_CODE_COMPLETION_PROMPT_GENERATORS
from evals.test_cases.chat_tests import CHAT_TESTS
from evals.test_cases.inline_code_completion_tests import INLINE_CODE_COMPLETION_TESTS
from evals.test_runners.utils import exec_code_and_get_globals_and_output
from evals.utils import get_script_from_cells, print_test_case_result_tables
from evals.asserts.equal_globals import assert_equal_globals, get_globals_to_compare

def run_chat_tests(test_name: Optional[str], prompt_name: Optional[str], tags: Optional[List[str]]):
    _run_code_gen_tests('chat', CHAT_TESTS, CHAT_PROMPT_GENERATORS, test_name, prompt_name, tags)

def run_inline_code_completion_tests(test_name: Optional[str], prompt_name: Optional[str], tags: Optional[List[str]]):
    _run_code_gen_tests('inline_code_completion', INLINE_CODE_COMPLETION_TESTS, INLINE_CODE_COMPLETION_PROMPT_GENERATORS, test_name, prompt_name, tags)

def _run_code_gen_tests(
    test_type: Literal['chat', 'inline_code_completion'],
    tests_cases: Union[List[ChatTestCase], List[InlineCodeCompletionTestCase]],
    prompt_generators: Union[List[ChatPromptGenerator], List[InlineCodeCompletionPromptGenerator]],
    test_name: Optional[str], 
    prompt_name: Optional[str], 
    tags: Optional[List[str]]
):
    print("Collecting tests...")
    tests_to_run = tests_cases
    if test_name:
        tests_to_run = [test for test in tests_to_run if test.name == test_name]
        if not tests_to_run:
            print(f"No test found with name: {test_name}")
            exit(1)

    if tags:
        tests_to_run = [test for test in tests_to_run if any(tag in tags for tag in test.test_case_core.workflow_tags)]
        if not tests_to_run:
            print(f"No tests found with tags: {tags}")
            exit(1)

    print(f"Collected {len(tests_to_run)} tests")

    # Filter prompts if prompt name provided
    print("Collecting prompts...")
    prompt_generators_to_test = prompt_generators
    if prompt_name:
        prompt_generators_to_test = [prompt for prompt in prompt_generators_to_test if prompt.prompt_name == prompt_name]
        if not prompt_generators_to_test:
            print(f"No prompt found with name: {prompt_name}")
            exit(1)
    print(f"Collected {len(prompt_generators_to_test)} prompts")

    # Mapping from prompt name to test results for each prompt we test
    test_case_results: Dict[str, List[TestCaseResult]] = {}
    for prompt_generator in prompt_generators_to_test:
        test_case_results[prompt_generator.prompt_name] = []
        for test in tests_to_run:
            test_case_result = run_code_gen_test(test, prompt_generator)
            test_case_results[prompt_generator.prompt_name].append(test_case_result)

    print_test_case_result_tables(test_case_results)

def run_code_gen_test(
        test: Union[ChatTestCase, InlineCodeCompletionTestCase], 
        prompt_generator: Union[ChatPromptGenerator, InlineCodeCompletionPromptGenerator]
) -> TestCaseResult:
    print(f"\n\033[1mRunning test: {test.name}\033[0m")

    # Get the script from the cells
    current_cell_contents_script = get_script_from_cells(test.test_case_core.notebook_state.cell_contents)

    expected_code = current_cell_contents_script + "\n" + test.test_case_core.expected_code

    # Construct the prompt 
    if isinstance(prompt_generator, ChatPromptGenerator):
        prompt = prompt_generator.get_prompt(test.user_input, test.test_case_core.notebook_state)
    else:
        prompt = prompt_generator.get_prompt(test.prefix or "", test.suffix or "", test.test_case_core.notebook_state)

    # Get the code generated by the LLM
    ai_generated_code = get_open_ai_completion(prompt)

    # Construct the actual code
    if isinstance(prompt_generator, ChatPromptGenerator):
        actual_code = current_cell_contents_script + "\n" + ai_generated_code
    else:
        # Run the post-processing function
        ai_generated_code = prompt_generator.post_process_output(ai_generated_code, test.prefix or "", test.suffix or "")

        # We always add a newline between the current_cell_contents and the prefix. 
        # But we don't add a newline between the prefix -> ai_generated_code -> suffix, 
        # because the inline code completion can occur in the middle of a line. 
        actual_code = current_cell_contents_script + "\n" + (test.prefix or "") + ai_generated_code + (test.suffix or "")

    # Execute the code and check if they produce the same results
    try:
        expected_globals, expected_output = exec_code_and_get_globals_and_output(expected_code)
        actual_globals, actual_output = exec_code_and_get_globals_and_output(actual_code)
    except Exception as e:
        # Fail early if we can't execute the code
        print(f"Failed to execute code with error: {e}")
        print(f"AI Generated Code: {ai_generated_code}")
        print(f"Actual Code: {actual_code}")
        print(f"Expected Code: {expected_code}")
        return TestCaseResult(test=test, passed=False)

    equal_globals = assert_equal_globals(expected_globals, actual_globals, test.test_case_core.variables_to_compare)
    equal_outputs = assert_equal_outputs(expected_output, actual_output)

    passed = equal_globals and equal_outputs

    if not passed:
        debug_failed_test_case(test, ai_generated_code, actual_code, expected_code, equal_globals, equal_outputs, expected_globals, actual_globals, expected_output, actual_output)

    return TestCaseResult(test=test, passed=passed)


def debug_failed_test_case(
        test: Union[ChatTestCase, InlineCodeCompletionTestCase],
        ai_generated_code: str, 
        actual_code: str,
        expected_code: str, 
        equal_globals: bool, 
        equal_outputs: bool, 
        expected_globals: Dict[str, str],
        actual_globals: Dict[str, str],
        expected_output: str,
        actual_output: str,
    ) -> None:


    print(f"AI Generated Code: {ai_generated_code}")
    print(f"Actual Code: {actual_code}")
    print(f"Expected Code: {expected_code}")
    print(f"Equal Globals: {equal_globals}")
    print(f"Equal Outputs: {equal_outputs}")
    if not equal_outputs:
        print(f"Expected Output: {expected_output}")
        print(f"Actual Output: {actual_output}")
